"""
PL_python_test
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from airflow.models import Variable
from astro import sql as aql
from astro.table import Table, Metadata
import pandas as pd
import pendulum


@aql.dataframe(task_id="extract_metadata")
def extract_metadata_func():
    import sys
    import json
    import os
    import time
    import platform
    import cpuinfo
    import psutil
    import pandas as pd
    
    # Get Python version information
    python_version = sys.version.split()[0]
    print(f"Python Version: {python_version}")
    
    # Check if running in Astronomer Cloud IDE
    cloud_ide = any('Astronomer Cloud IDE' in arg for arg in sys.argv)
    print(f"Running in Astronomer Cloud IDE: {cloud_ide}")
    
    airflow_env_list = []
    for k, v in os.environ.items():
        if 'airflow' in k.lower():
            s = f"{k} = {v}"
            airflow_env_list.append(s)
    
    airflow_env_str = ", ".join(airflow_env_list)
    
    swap_total =  psutil.swap_memory().total
    swap_used = psutil.swap_memory().used
    if swap_total:
        swap_usage = swap_used / swap_total * 100
    else:
        swap_usage = 0
    
    # Additional metadata
    metadata = {
        "python_version": f"Python {python_version}",
        "cloud_ide": cloud_ide,
        "platform": platform.system(),
        "platform_release": platform.release(),
        "platform_version": platform.version(),
        "machine_type": platform.machine(),
        "processor_type": cpuinfo.get_cpu_info()['brand_raw'],
        "working_directory": os.getcwd(),
        "command_line_arguments": ", ".join(sys.argv),
        "env_variables": airflow_env_str,
        "module_search_paths": os.environ['PATH'],
        "cpu_usage": psutil.cpu_percent(interval=1),
        "memory_usage": psutil.virtual_memory().percent,
        "swap_usage": swap_usage,
        "disk_usage": psutil.disk_usage('/').percent
    }
    
    df = pd.DataFrame(metadata, index=[0])
    return df
    
    

@aql.run_raw_sql(conn_id="duckdb_default", task_id="interim_sql", results_format="pandas_dataframe")
def interim_sql_func(extract_metadata: Table):
    return """
    SELECT * FROM {{extract_metadata}}
    """

@aql.dataframe(task_id="load_metadata")
def load_metadata_func(extract_metadata: pd.DataFrame):
    from sqlalchemy import create_engine
    
    username = Variable.get('USERNAME')
    password = Variable.get('PASSWORD')
    host = Variable.get('HOST')
    database = Variable.get('DATABASE')
    table_name = Variable.get('TABLE_NAME')
    conn_string = f"mssql+pyodbc://{username}:{password}@{host}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server"
    engine = create_engine(conn_string, echo=True)
    
    df = extract_metadata
    df.to_sql(name=table_name, con=engine, if_exists='append', index=False)

default_args={
    "email_on_retry": True,
    "retries": 2,
    "owner": "Siddhardha Madda,Open in Cloud IDE",
}

@dag(
    default_args=default_args,
    schedule="10-20/5 10-11 * * *",
    start_date=pendulum.from_format("2024-02-09", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
    owner_links={
        "Siddhardha Madda": "mailto:siddhardha.madda@acadiahealthcare.com",
        "Open in Cloud IDE": "https://cloud.astronomer.io/clrpl8m6r01hx01mp5w9ydyy9/cloud-ide/clrxr6ls1002u01kuqliasul6/clserx0sw04pu01qjhhzj14qe",
    },
)
def PL_python_test():
    extract_metadata = extract_metadata_func()

    interim_sql = interim_sql_func(
        extract_metadata,
    )

    load_metadata = load_metadata_func(
        extract_metadata,
    )

    interim_sql << extract_metadata

    load_metadata << extract_metadata

dag_obj = PL_python_test()

